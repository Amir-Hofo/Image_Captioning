{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-bxFr6lwOLjd",
        "1n8COqkTPk_U",
        "G7jzMUZvPwWr",
        "ls2RGUE5QD_S",
        "NZaMPk6DQZlw",
        "cf1ZtDFgQdy7",
        "e0mXIcwQQmwD",
        "moAfdzqsQpyf",
        "HdP2j7etQxa4",
        "tSXKhe7_jrSQ",
        "DzEb0gpWmmRo",
        "NAkYf7RxxM7K",
        "E48f9Ly95CWB",
        "dSHoNHxz6vMD",
        "4j746xdP7dnb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Amir-Hofo"
      ],
      "metadata": {
        "id": "XHuBUyv2ON_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------"
      ],
      "metadata": {
        "id": "njTeklk6OLy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 00_Arguments"
      ],
      "metadata": {
        "id": "-bxFr6lwOLjd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OuqnqALZNf0i"
      },
      "outputs": [],
      "source": [
        "system= \"colab\"\n",
        "grid_search= True\n",
        "seed= False\n",
        "\n",
        "wandb_enable= False\n",
        "if wandb_enable:\n",
        "  wandb_arg_name= input('Please input the WandB argument name:')\n",
        "\n",
        "batch_size= None\n",
        "seq_len= None\n",
        "\n",
        "embedding_dim= None\n",
        "num_layers= None\n",
        "hidden_dim= None\n",
        "weight_drop= None\n",
        "\n",
        "lr= None\n",
        "wd= None\n",
        "momentum= None\n",
        "clip= None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------"
      ],
      "metadata": {
        "id": "Pmr9P_5xPaXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01_Library"
      ],
      "metadata": {
        "id": "OHtHolGIPbef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## insatll"
      ],
      "metadata": {
        "id": "zPap2nnEPewg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python --version\n",
        "import torch\n",
        "for lib in [torch, torchvision]:\n",
        "  print(lib.__name__, '-->', lib.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRmQjvH_CRSW",
        "outputId": "92b282c5-21b3-448e-b51a-cf0b12dbeeb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n",
            "torch --> 2.6.0+cu124\n",
            "torchvision --> 0.21.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torchtext\n",
        "except ImportError:\n",
        "    ! pip install -q torchtext==0.17.0\n",
        "    import torchtext\n",
        "\n",
        "try:\n",
        "    import torchvision\n",
        "except (ImportError, OSError):\n",
        "    ! pip uninstall -q -y torchvision\n",
        "    ! pip install -q torchvision==0.17.0\n",
        "    import torchvision\n",
        "\n",
        "! pip install -q torchmetrics tqdm wandb"
      ],
      "metadata": {
        "id": "ggsQdz1CPa_2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "c7b904e6-77ff-4aa5-abbc-fae7e15539d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.library' has no attribute 'register_fake'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae639952cee5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pip uninstall -q -y torchvision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"boxes should be a 2d tensor, got {dets.dim()}D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.library' has no attribute 'register_fake'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "1n8COqkTPk_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import wandb\n",
        "import tqdm\n",
        "import torchmetrics as tm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "9MKSoa5xPmB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python --version\n",
        "for lib in [torch, torchtext, torchvision]:\n",
        "  print(lib.__name__, '-->', lib.__version__)"
      ],
      "metadata": {
        "id": "jxpP3bCFdgwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------"
      ],
      "metadata": {
        "id": "pc5B9M0_Pvcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02_Utils"
      ],
      "metadata": {
        "id": "G7jzMUZvPwWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## system"
      ],
      "metadata": {
        "id": "ls2RGUE5QD_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if system== \"local\":\n",
        "    project_path= r\"./\"\n",
        "    dataset_path= './dataset/'\n",
        "\n",
        "elif system== \"colab\":\n",
        "    root_path= '/content/'\n",
        "    project_path= r\"/content/drive/MyDrive/Catalist/2_image captioning/\"\n",
        "    dataset_path= os.path.join(project_path, r'dataset/')\n",
        "\n",
        "else:\n",
        "  raise ValueError(\"Invalid system\")"
      ],
      "metadata": {
        "id": "Aergrhk4Pv6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## device"
      ],
      "metadata": {
        "id": "NZaMPk6DQZlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "Np3fZxbWQc_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drive mount"
      ],
      "metadata": {
        "id": "cf1ZtDFgQdy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if system== \"colab\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "crV_FdPoQhOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## number of params fn"
      ],
      "metadata": {
        "id": "e0mXIcwQQmwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_trainable_params(model):\n",
        "  nums= sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "  return nums"
      ],
      "metadata": {
        "id": "bNg45ZseQnH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## avragemeter"
      ],
      "metadata": {
        "id": "moAfdzqsQpyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "3sdAg9hSQuKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set seed"
      ],
      "metadata": {
        "id": "HdP2j7etQxa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "SCLHipXWQxMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------"
      ],
      "metadata": {
        "id": "aFYm-HmbQ7lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03_Data"
      ],
      "metadata": {
        "id": "c-N697hcQ82f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download dataset"
      ],
      "metadata": {
        "id": "tSXKhe7_jrSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_link= \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
        "caption_link= \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
        "\n",
        "files = {\n",
        "    \"Flickr8k_Dataset.zip\": image_link,\n",
        "    \"Flickr8k_text.zip\": caption_link\n",
        "}\n",
        "\n",
        "for filename, url in files.items():\n",
        "  if not os.path.exists(os.path.join(dataset_path, filename)):\n",
        "    urllib.request.urlretrieve(url, os.path.join(dataset_path, filename))\n",
        "    print(f\"{filename} has been downloaded.\")\n",
        "  else:\n",
        "    print(f\"{filename} already exists.\")"
      ],
      "metadata": {
        "id": "ALt2fNrNjq2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unzip"
      ],
      "metadata": {
        "id": "DzEb0gpWmmRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files= [\"Flickr8k_Dataset.zip\", \"Flickr8k_text.zip\"]\n",
        "data_path= os.path.join(root_path, \"dataset\")\n",
        "os.makedirs(data_path, exist_ok= True)\n",
        "for file in files:\n",
        "    with zipfile.ZipFile(os.path.join(dataset_path, file), 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "    print(f\"{file} extraction is complete.\")"
      ],
      "metadata": {
        "id": "nLCPWgv9mpLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## custom dataset"
      ],
      "metadata": {
        "id": "sAdfKyMLl3E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pytorch Flickr30k class"
      ],
      "metadata": {
        "id": "NAkYf7RxxM7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import glob\n",
        "# import os\n",
        "# from collections import defaultdict\n",
        "# from html.parser import HTMLParser\n",
        "# from pathlib import Path\n",
        "# from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "# from .folder import default_loader\n",
        "# from .vision import VisionDataset\n",
        "\n",
        "\n",
        "# class Flickr30k(VisionDataset):\n",
        "#     \"\"\"`Flickr30k Entities <https://bryanplummer.com/Flickr30kEntities/>`_ Dataset.\n",
        "\n",
        "#     Args:\n",
        "#         root (str or ``pathlib.Path``): Root directory where images are downloaded to.\n",
        "#         ann_file (string): Path to annotation file.\n",
        "#         transform (callable, optional): A function/transform that takes in a PIL image or torch.Tensor, depends on the given loader,\n",
        "#             and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "#         target_transform (callable, optional): A function/transform that takes in the\n",
        "#             target and transforms it.\n",
        "#         loader (callable, optional): A function to load an image given its path.\n",
        "#             By default, it uses PIL as its image loader, but users could also pass in\n",
        "#             ``torchvision.io.decode_image`` for decoding image data into tensors directly.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         root: str,\n",
        "#         ann_file: str,\n",
        "#         transform: Optional[Callable] = None,\n",
        "#         target_transform: Optional[Callable] = None,\n",
        "#         loader: Callable[[str], Any] = default_loader,\n",
        "#     ) -> None:\n",
        "#         super().__init__(root, transform=transform, target_transform=target_transform)\n",
        "#         self.ann_file = os.path.expanduser(ann_file)\n",
        "\n",
        "#         # Read annotations and store in a dict\n",
        "#         self.annotations = defaultdict(list)\n",
        "#         with open(self.ann_file) as fh:\n",
        "#             for line in fh:\n",
        "#                 img_id, caption = line.strip().split(\"\\t\")\n",
        "#                 self.annotations[img_id[:-2]].append(caption)\n",
        "\n",
        "#         self.ids = list(sorted(self.annotations.keys()))\n",
        "#         self.loader = loader\n",
        "\n",
        "#     def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             index (int): Index\n",
        "\n",
        "#         Returns:\n",
        "#             tuple: Tuple (image, target). target is a list of captions for the image.\n",
        "#         \"\"\"\n",
        "#         img_id = self.ids[index]\n",
        "\n",
        "#         # Image\n",
        "#         filename = os.path.join(self.root, img_id)\n",
        "#         img = self.loader(filename)\n",
        "#         if self.transform is not None:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         # Captions\n",
        "#         target = self.annotations[img_id]\n",
        "#         if self.target_transform is not None:\n",
        "#             target = self.target_transform(target)\n",
        "\n",
        "#         return img, target\n",
        "\n",
        "#     def __len__(self) -> int:\n",
        "#         return len(self.ids)"
      ],
      "metadata": {
        "id": "nWONWtmBQ8QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### custom Flickr dataset"
      ],
      "metadata": {
        "id": "s1k90Mk3xVlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr8k(VisionDataset):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        ann_file (string): Path to annotation file.\n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.PILToTensor``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 root: str,\n",
        "                 ann_file: str,\n",
        "                 split_file: str,\n",
        "                 train: bool,\n",
        "                 transform: Optional[Callable]= None,\n",
        "                 target_transform: Optional[Callable]= None)\n",
        "        super().__init__(root, transform= transform,\n",
        "                         target_transform= target_transform)\n",
        "        self.ann_file= os.path.expanduser(ann_file)\n",
        "        self.train= train\n",
        "\n",
        "        # Read {train/dev/test} files\n",
        "        with open(split_file) as f:\n",
        "            self.split_samples= f.read().strip().split(\"\\n\")\n",
        "\n",
        "        # Read annotations and store in a dict\n",
        "        self.ids, self.captions= [], []\n",
        "        with open(self.ann_file) as fh:\n",
        "            for line in fh:\n",
        "                img_id, caption= line.strip().split(\"\\t\")\n",
        "                if img_id[:-2] in self.split_samples:\n",
        "                    self.ids.append(img_id[:-2])\n",
        "                    self.captions.append(caption)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple (image, target). target is a list of captions for the image.\n",
        "        \"\"\"\n",
        "        img_id= self.ids[index]\n",
        "\n",
        "        # Image\n",
        "        filename= os.path.join(self.root, img_id)\n",
        "        img_raw= Image.open(filename).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            img= self.transform(img_raw)\n",
        "\n",
        "        # Captions\n",
        "        caption= self.captions[index]\n",
        "        if self.target_transform is not None:\n",
        "            target= self.target_transform(caption)\n",
        "\n",
        "        if self.train:\n",
        "            return img, target\n",
        "        else:\n",
        "          return img, img_raw, caption\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)"
      ],
      "metadata": {
        "id": "u26czZqWsLrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### caption transform"
      ],
      "metadata": {
        "id": "E48f9Ly95CWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionTransform:\n",
        "\n",
        "    def __init__(self, caption_file):\n",
        "        captions= self._load_captions(caption_file)\n",
        "\n",
        "        self.tokenizer= get_tokenizer('basic_english')\n",
        "        self.vocab= build_vocab_from_iterator(map(self.tokenizer, captions),\n",
        "                                              specials= ['<pad>', '<unk>', '<sos>', '<eos>'])\n",
        "        self.vocab.set_default_index(self.vocab['<unk>'])\n",
        "        torch.save(self.vocab, 'vocab.pt')\n",
        "\n",
        "    def __call__(self, caption):\n",
        "        indices= self.vocab(self.tokenizer(caption))\n",
        "        indices= self.vocab(['<sos>']) + indices + self.vocab(['<eos>'])\n",
        "        target= torch.LongTensor(indices)\n",
        "        return target\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"CaptionTransform([\n",
        "          _load_captions(),\n",
        "          toknizer('basic_english'),\n",
        "          vocab(vocab_size={len(self.vocab)}) ])\n",
        "          \"\"\"\n",
        "\n",
        "    def _load_captions(self, caption_file):\n",
        "        captions= []\n",
        "        with open(caption_file) as f:\n",
        "            for line in f:\n",
        "                _, caption= line.strip().split(\"\\t\")\n",
        "                captions.append(caption)\n",
        "        return captions"
      ],
      "metadata": {
        "id": "O9ps5Z4a5Fad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_transform= CaptionTransform(os.path.join(data_path, 'Flickr8k.token.txt'))"
      ],
      "metadata": {
        "id": "sBJh7vb76XSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transform"
      ],
      "metadata": {
        "id": "dSHoNHxz6vMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform= transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
        "\n",
        "eval_transform= transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])"
      ],
      "metadata": {
        "id": "MkaxJtXF6-Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "4j746xdP7dnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_file= lambda phase: f'{data_path}Flickr_8k.{phase}Images.txt'\n",
        "\n",
        "train_set= Flickr8k(root, ann_file, split_file('train'),\n",
        "                    True, train_transform, caption_transform)\n",
        "valid_set= Flickr8k(root, ann_file, split_file('dev'),\n",
        "                    True, eval_transform, caption_transform)\n",
        "test_set= Flickr8k(root, ann_file, split_file('test'),\n",
        "                   False, eval_transform, caption_transform)\n",
        "\n",
        "len(train_set), len(valid_set), len(test_set)"
      ],
      "metadata": {
        "id": "rFhRUC-L7Fqy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}