{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-bxFr6lwOLjd",
        "OHtHolGIPbef",
        "zPap2nnEPewg",
        "1n8COqkTPk_U",
        "G7jzMUZvPwWr",
        "ls2RGUE5QD_S",
        "NZaMPk6DQZlw",
        "cf1ZtDFgQdy7",
        "e0mXIcwQQmwD",
        "moAfdzqsQpyf",
        "HdP2j7etQxa4",
        "c-N697hcQ82f",
        "tSXKhe7_jrSQ",
        "DzEb0gpWmmRo",
        "sAdfKyMLl3E-",
        "NAkYf7RxxM7K",
        "s1k90Mk3xVlq",
        "E48f9Ly95CWB",
        "dSHoNHxz6vMD",
        "4j746xdP7dnb",
        "gTKii0n7tTdt",
        "RNkZsDrkxzRF",
        "Lmr1L9Z9_zZc",
        "vfbwtbuGBG0b",
        "-eaXcN7kC6PL",
        "CPILawUCDzfs",
        "EENenCM1D3Xt",
        "xk-OgRNgtoTF"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Amir-Hofo"
      ],
      "metadata": {
        "id": "XHuBUyv2ON_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------"
      ],
      "metadata": {
        "id": "njTeklk6OLy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 00_Arguments"
      ],
      "metadata": {
        "id": "-bxFr6lwOLjd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OuqnqALZNf0i"
      },
      "outputs": [],
      "source": [
        "system= \"colab\"\n",
        "grid_search= True\n",
        "seed= False\n",
        "\n",
        "wandb_enable= False\n",
        "if wandb_enable:\n",
        "  wandb_arg_name= input('Please input the WandB argument name:')\n",
        "\n",
        "batch_size= 128\n",
        "max_seq_length= 20\n",
        "\n",
        "embed_size= 300\n",
        "num_layers= 2\n",
        "hidden_size= 500\n",
        "dropout_embd= 0.5\n",
        "dropout_rnn= 0.5\n",
        "\n",
        "\n",
        "lr= 0.1\n",
        "wd= 1e-4\n",
        "momentum= 0.9\n",
        "clip= 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------"
      ],
      "metadata": {
        "id": "Pmr9P_5xPaXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01_Library"
      ],
      "metadata": {
        "id": "OHtHolGIPbef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## insatll"
      ],
      "metadata": {
        "id": "zPap2nnEPewg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torchtext\n",
        "except ImportError:\n",
        "    ! pip install -q torchtext==0.17.0\n",
        "    import torchtext\n",
        "\n",
        "try:\n",
        "    import torchvision\n",
        "except:\n",
        "    ! pip uninstall -q -y torchvision\n",
        "    ! pip install -q torchvision==0.17.0\n",
        "    import torchvision\n",
        "\n",
        "! pip install -q torchmetrics tqdm wandb torcheval"
      ],
      "metadata": {
        "id": "ggsQdz1CPa_2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -q -y torch torchvision torchtext\n",
        "# !pip install -q torch=2.2.2 torchtext==0.17.2 torchvision==0.17.2"
      ],
      "metadata": {
        "id": "FoBJAi1Zlm_M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "1n8COqkTPk_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import wandb\n",
        "import tqdm\n",
        "import torchmetrics as tm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "try:\n",
        "    from torcheval.metrics import BLEUScore\n",
        "except:\n",
        "    import torcheval\n",
        "    from bleu import BLEUScore"
      ],
      "metadata": {
        "id": "9MKSoa5xPmB4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python --version\n",
        "for lib in [torch, torchtext, torchvision]:\n",
        "  print(lib.__name__, '-->', lib.__version__)"
      ],
      "metadata": {
        "id": "jxpP3bCFdgwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fdf631-21ed-4ea2-c17a-9d97cc20ceaa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n",
            "torch --> 2.2.0+cu121\n",
            "torchtext --> 0.17.0+cpu\n",
            "torchvision --> 0.17.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------"
      ],
      "metadata": {
        "id": "pc5B9M0_Pvcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02_Utils"
      ],
      "metadata": {
        "id": "G7jzMUZvPwWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## system"
      ],
      "metadata": {
        "id": "ls2RGUE5QD_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if system== \"local\":\n",
        "    project_path= r\"./\"\n",
        "    dataset_path= './dataset/'\n",
        "\n",
        "elif system== \"colab\":\n",
        "    root_path= '/content/'\n",
        "    project_path= r\"/content/drive/MyDrive/Catalist/2_image captioning/\"\n",
        "    dataset_path= os.path.join(project_path, r'dataset/')\n",
        "\n",
        "else:\n",
        "  raise ValueError(\"Invalid system\")"
      ],
      "metadata": {
        "id": "Aergrhk4Pv6W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## device"
      ],
      "metadata": {
        "id": "NZaMPk6DQZlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pin_memory= (device == 'cuda')\n",
        "device"
      ],
      "metadata": {
        "id": "Np3fZxbWQc_G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "4caadf51-1e8f-42cf-cb1a-47c82c15f622"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drive mount"
      ],
      "metadata": {
        "id": "cf1ZtDFgQdy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if system== \"colab\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "crV_FdPoQhOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e79435-9dc1-44f5-dee5-ed6e994bd1b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## number of params fn"
      ],
      "metadata": {
        "id": "e0mXIcwQQmwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_trainable_params(model):\n",
        "  nums= sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "  return nums"
      ],
      "metadata": {
        "id": "bNg45ZseQnH-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## avragemeter"
      ],
      "metadata": {
        "id": "moAfdzqsQpyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "3sdAg9hSQuKu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set seed"
      ],
      "metadata": {
        "id": "HdP2j7etQxa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "SCLHipXWQxMj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------"
      ],
      "metadata": {
        "id": "aFYm-HmbQ7lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03_Data"
      ],
      "metadata": {
        "id": "c-N697hcQ82f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download dataset"
      ],
      "metadata": {
        "id": "tSXKhe7_jrSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_link= \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
        "caption_link= \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
        "\n",
        "files = {\n",
        "    \"Flickr8k_Dataset.zip\": image_link,\n",
        "    \"Flickr8k_text.zip\": caption_link\n",
        "}\n",
        "\n",
        "for filename, url in files.items():\n",
        "  if not os.path.exists(os.path.join(dataset_path, filename)):\n",
        "    urllib.request.urlretrieve(url, os.path.join(dataset_path, filename))\n",
        "    print(f\"{filename} has been downloaded.\")\n",
        "  else:\n",
        "    print(f\"{filename} already exists.\")"
      ],
      "metadata": {
        "id": "ALt2fNrNjq2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ee92b5-3f6a-419e-cc58-4b51dd0fd8e5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flickr8k_Dataset.zip already exists.\n",
            "Flickr8k_text.zip already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unzip"
      ],
      "metadata": {
        "id": "DzEb0gpWmmRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files= [\"Flickr8k_Dataset.zip\", \"Flickr8k_text.zip\"]\n",
        "data_path= os.path.join(root_path, \"dataset/\")\n",
        "os.makedirs(data_path, exist_ok= True)\n",
        "for file in files:\n",
        "    with zipfile.ZipFile(os.path.join(dataset_path, file), 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "    print(f\"{file} extraction is complete.\")"
      ],
      "metadata": {
        "id": "nLCPWgv9mpLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0f2e86-d717-43ca-e676-7216e3c32be0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flickr8k_Dataset.zip extraction is complete.\n",
            "Flickr8k_text.zip extraction is complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## custom dataset"
      ],
      "metadata": {
        "id": "sAdfKyMLl3E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pytorch Flickr30k class"
      ],
      "metadata": {
        "id": "NAkYf7RxxM7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import glob\n",
        "# import os\n",
        "# from collections import defaultdict\n",
        "# from html.parser import HTMLParser\n",
        "# from pathlib import Path\n",
        "# from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "# from .folder import default_loader\n",
        "# from .vision import VisionDataset\n",
        "\n",
        "\n",
        "# class Flickr30k(VisionDataset):\n",
        "#     \"\"\"`Flickr30k Entities <https://bryanplummer.com/Flickr30kEntities/>`_ Dataset.\n",
        "\n",
        "#     Args:\n",
        "#         root (str or ``pathlib.Path``): Root directory where images are downloaded to.\n",
        "#         ann_file (string): Path to annotation file.\n",
        "#         transform (callable, optional): A function/transform that takes in a PIL image or torch.Tensor, depends on the given loader,\n",
        "#             and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "#         target_transform (callable, optional): A function/transform that takes in the\n",
        "#             target and transforms it.\n",
        "#         loader (callable, optional): A function to load an image given its path.\n",
        "#             By default, it uses PIL as its image loader, but users could also pass in\n",
        "#             ``torchvision.io.decode_image`` for decoding image data into tensors directly.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         root: str,\n",
        "#         ann_file: str,\n",
        "#         transform: Optional[Callable] = None,\n",
        "#         target_transform: Optional[Callable] = None,\n",
        "#         loader: Callable[[str], Any] = default_loader,\n",
        "#     ) -> None:\n",
        "#         super().__init__(root, transform=transform, target_transform=target_transform)\n",
        "#         self.ann_file = os.path.expanduser(ann_file)\n",
        "\n",
        "#         # Read annotations and store in a dict\n",
        "#         self.annotations = defaultdict(list)\n",
        "#         with open(self.ann_file) as fh:\n",
        "#             for line in fh:\n",
        "#                 img_id, caption = line.strip().split(\"\\t\")\n",
        "#                 self.annotations[img_id[:-2]].append(caption)\n",
        "\n",
        "#         self.ids = list(sorted(self.annotations.keys()))\n",
        "#         self.loader = loader\n",
        "\n",
        "#     def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             index (int): Index\n",
        "\n",
        "#         Returns:\n",
        "#             tuple: Tuple (image, target). target is a list of captions for the image.\n",
        "#         \"\"\"\n",
        "#         img_id = self.ids[index]\n",
        "\n",
        "#         # Image\n",
        "#         filename = os.path.join(self.root, img_id)\n",
        "#         img = self.loader(filename)\n",
        "#         if self.transform is not None:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         # Captions\n",
        "#         target = self.annotations[img_id]\n",
        "#         if self.target_transform is not None:\n",
        "#             target = self.target_transform(target)\n",
        "\n",
        "#         return img, target\n",
        "\n",
        "#     def __len__(self) -> int:\n",
        "#         return len(self.ids)"
      ],
      "metadata": {
        "id": "nWONWtmBQ8QO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### custom Flickr dataset"
      ],
      "metadata": {
        "id": "s1k90Mk3xVlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr8k(VisionDataset):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        ann_file (string): Path to annotation file.\n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.PILToTensor``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 root: str,\n",
        "                 ann_file: str,\n",
        "                 split_file: str,\n",
        "                 train: bool,\n",
        "                 transform: Optional[Callable]= None,\n",
        "                 target_transform: Optional[Callable]= None):\n",
        "        super().__init__(root, transform= transform,\n",
        "                         target_transform= target_transform)\n",
        "        self.ann_file= os.path.expanduser(ann_file)\n",
        "        self.train= train\n",
        "\n",
        "        # Read {train/dev/test} files\n",
        "        with open(split_file) as f:\n",
        "            self.split_samples= f.read().strip().split(\"\\n\")\n",
        "\n",
        "        # Read annotations and store in a dict\n",
        "        self.ids, self.captions= [], []\n",
        "        with open(self.ann_file) as fh:\n",
        "            for line in fh:\n",
        "                img_id, caption= line.strip().split(\"\\t\")\n",
        "                if img_id[:-2] in self.split_samples:\n",
        "                    self.ids.append(img_id[:-2])\n",
        "                    self.captions.append(caption)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple (image, target). target is a list of captions for the image.\n",
        "        \"\"\"\n",
        "        img_id= self.ids[index]\n",
        "\n",
        "        # Image\n",
        "        filename= os.path.join(self.root, img_id)\n",
        "        img_raw= Image.open(filename).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            img= self.transform(img_raw)\n",
        "\n",
        "        # Captions\n",
        "        caption= self.captions[index]\n",
        "        if self.target_transform is not None:\n",
        "            target= self.target_transform(caption)\n",
        "\n",
        "        if self.train:\n",
        "            return img, target\n",
        "        else:\n",
        "          return img, img_raw, caption\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)"
      ],
      "metadata": {
        "id": "u26czZqWsLrr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### caption transform"
      ],
      "metadata": {
        "id": "E48f9Ly95CWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionTransform:\n",
        "\n",
        "    def __init__(self, caption_file):\n",
        "        captions= self._load_captions(caption_file)\n",
        "\n",
        "        self.tokenizer= get_tokenizer('basic_english')\n",
        "        self.vocab= build_vocab_from_iterator(map(self.tokenizer, captions),\n",
        "                                              specials= ['<pad>', '<unk>', '<sos>', '<eos>'])\n",
        "        self.vocab.set_default_index(self.vocab['<unk>'])\n",
        "        torch.save(self.vocab, 'vocab.pt')\n",
        "\n",
        "    def __call__(self, caption):\n",
        "        indices= self.vocab(self.tokenizer(caption))\n",
        "        indices= self.vocab(['<sos>']) + indices + self.vocab(['<eos>'])\n",
        "        target= torch.LongTensor(indices)\n",
        "        return target\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"CaptionTransform([\n",
        "          _load_captions(),\n",
        "          toknizer('basic_english'),\n",
        "          vocab(vocab_size={len(self.vocab)}) ])\n",
        "          \"\"\"\n",
        "\n",
        "    def _load_captions(self, caption_file):\n",
        "        captions= []\n",
        "        with open(caption_file) as f:\n",
        "            for line in f:\n",
        "                _, caption= line.strip().split(\"\\t\")\n",
        "                captions.append(caption)\n",
        "        return captions"
      ],
      "metadata": {
        "id": "O9ps5Z4a5Fad"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_transform= CaptionTransform(os.path.join(data_path, 'Flickr8k.token.txt'))"
      ],
      "metadata": {
        "id": "sBJh7vb76XSG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transform"
      ],
      "metadata": {
        "id": "dSHoNHxz6vMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform= transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
        "\n",
        "eval_transform= transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])"
      ],
      "metadata": {
        "id": "MkaxJtXF6-Xf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "4j746xdP7dnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_file= lambda phase: f'{data_path}Flickr_8k.{phase}Images.txt'\n",
        "\n",
        "train_set= Flickr8k(os.path.join(data_path, 'Flicker8k_Dataset'),\n",
        "                    os.path.join(data_path, 'Flickr8k.token.txt'),\n",
        "                    split_file('train'), True,\n",
        "                    train_transform, caption_transform)\n",
        "\n",
        "valid_set= Flickr8k(os.path.join(data_path, 'Flicker8k_Dataset'),\n",
        "                    os.path.join(data_path, 'Flickr8k.token.txt'),\n",
        "                    split_file('dev'), True,\n",
        "                    eval_transform, caption_transform)\n",
        "\n",
        "test_set= Flickr8k(os.path.join(data_path, 'Flicker8k_Dataset'),\n",
        "                   os.path.join(data_path, 'Flickr8k.token.txt'),\n",
        "                   split_file('test'), False,\n",
        "                   eval_transform, caption_transform)\n",
        "\n",
        "len(train_set), len(valid_set), len(test_set)"
      ],
      "metadata": {
        "id": "rFhRUC-L7Fqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c7789d-c74d-4f8e-9454-31f29412247f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 5000, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataloader"
      ],
      "metadata": {
        "id": "gTKii0n7tTdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  if len(batch[0]) == 2:\n",
        "      x_batch, y_batch= zip(*batch)\n",
        "      x_batch= torch.stack(x_batch)\n",
        "      y_batch= pad_sequence(y_batch, batch_first= True,\n",
        "                            padding_value= caption_transform.vocab['<pad>'])\n",
        "      return x_batch, y_batch\n",
        "  else:\n",
        "    x_batch, x_raw, captions= zip(*batch)\n",
        "    x_batch= torch.stack(x_batch)\n",
        "    return x_batch, x_raw, captions"
      ],
      "metadata": {
        "id": "iNoZvVKHtUqk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader= DataLoader(train_set, batch_size= batch_size,\n",
        "                         shuffle= True, collate_fn= collate_fn,\n",
        "                         pin_memory= pin_memory)\n",
        "valid_loader= DataLoader(valid_set, batch_size= batch_size*2,\n",
        "                         collate_fn= collate_fn, pin_memory= pin_memory)\n",
        "test_loader= DataLoader(test_set, batch_size= batch_size*2,\n",
        "                        collate_fn= collate_fn, pin_memory= pin_memory)\n",
        "\n",
        "print(\"train batch size:\",train_loader.batch_size,\n",
        "     \", num of batch:\", len(train_loader))\n",
        "print(\"valid batch size:\",valid_loader.batch_size,\n",
        "     \", num of batch:\", len(valid_loader))\n",
        "print(\"Test batch size:\",test_loader.batch_size,\n",
        "     \", num of batch:\", len(test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edpA5CGAw1z0",
        "outputId": "e6c26158-6e1a-4318-e8d0-1f2335ca8a80"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train batch size: 128 , num of batch: 235\n",
            "valid batch size: 256 , num of batch: 20\n",
            "Test batch size: 256 , num of batch: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------"
      ],
      "metadata": {
        "id": "sOvriHOYxwlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04_Model"
      ],
      "metadata": {
        "id": "RNkZsDrkxzRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## encoder"
      ],
      "metadata": {
        "id": "Lmr1L9Z9_zZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size):\n",
        "    super(EncoderCNN, self).__init__()\n",
        "    self.resnet= resnet50(weights= ResNet50_Weights.IMAGENET1K_V2)\n",
        "    self.resnet.requires_grad_(False)\n",
        "    feature_size= self.resnet.fc.in_features\n",
        "\n",
        "    self.resnet.fc= nn.Identity()\n",
        "    self.linear= nn.Linear(feature_size, embed_size)\n",
        "    self.bn= nn.BatchNorm1d(embed_size)\n",
        "\n",
        "  def forward(self, images):\n",
        "    self.resnet.eval()\n",
        "    with torch.no_grad():\n",
        "      features= self.resnet(images)\n",
        "    features= self.bn(self.linear(features))\n",
        "    return features"
      ],
      "metadata": {
        "id": "NEaDAA-zxxhy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## decoder"
      ],
      "metadata": {
        "id": "vfbwtbuGBG0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers,\n",
        "               dropout_embd, dropout_rnn, max_seq_length= 20):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.embedding= nn.Embedding(vocab_size, embed_size,\n",
        "                                 padding_idx= caption_transform.vocab['<pad>'])\n",
        "    self.dropout_embd= nn.Dropout(dropout_embd)\n",
        "\n",
        "    self.lstm= nn.LSTM(embed_size, hidden_size, num_layers,\n",
        "                       dropout= dropout_rnn, batch_first= True)\n",
        "\n",
        "    self.linear= nn.Linear(hidden_size, vocab_size)\n",
        "    self.max_seq_length= max_seq_length\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "      self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "      self.linear.bias.data.fill_(0)\n",
        "      self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embeddings= self.dropout_embd(self.embedding(captions[:, :-1]))\n",
        "    inputs= torch.cat((features.unsqueeze(1), embeddings), dim= 1)\n",
        "    outputs, _= self.lstm(inputs)\n",
        "    outputs= self.linear(outputs)\n",
        "    return outputs\n",
        "\n",
        "  def generate(self, features, captions):\n",
        "    if len(captions) > 0:\n",
        "        embeddings= self.dropout_embd(self.embedding(captions))\n",
        "        inputs= torch.cat((features.unsqueeze(1), embeddings), dim= 1)\n",
        "    else:\n",
        "        inputs= features.unsqueeze(1)\n",
        "\n",
        "    outputs, _= self.lstm(inputs)\n",
        "    outputs= self.linear(outputs)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "V_B2hQ0nBSMN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## custom model"
      ],
      "metadata": {
        "id": "-eaXcN7kC6PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioning(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers,\n",
        "               dropout_embd, dropout_rnn, max_seq_length= 20):\n",
        "    super(ImageCaptioning, self).__init__()\n",
        "    self.encoder= EncoderCNN(embed_size)\n",
        "    self.decoder= DecoderRNN(embed_size, hidden_size, vocab_size, num_layers,\n",
        "                             dropout_embd, dropout_rnn, max_seq_length)\n",
        "\n",
        "  def forward(self, images, captions):\n",
        "    features= self.encoder(images)\n",
        "    outputs= self.decoder(features, captions)\n",
        "    return outputs\n",
        "\n",
        "  def generate(self, images, captions):\n",
        "    features= self.encoder(images)\n",
        "    outputs= self.decoder.generate(features, captions)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "NaP8lDXsDEMM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## configuration"
      ],
      "metadata": {
        "id": "CPILawUCDzfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn= nn.CrossEntropyLoss(ignore_index= caption_transform.vocab['<pad>'])"
      ],
      "metadata": {
        "id": "dbnV1cJ_D1Or"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------"
      ],
      "metadata": {
        "id": "mt9sC5_LD18A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train one epoch"
      ],
      "metadata": {
        "id": "EENenCM1D3Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, loss_fn, optimizer, metric= None, epoch= None):\n",
        "  model.train()\n",
        "  loss_train= AverageMeter()\n",
        "  if metric: metric.reset()\n",
        "\n",
        "  with tqdm.tqdm(train_loader, unit= 'batch') as tepoch:\n",
        "    for inputs, targets in tepoch:\n",
        "      if epoch:\n",
        "        tepoch.set_description(f'Epoch {epoch}')\n",
        "\n",
        "      inputs, targets= inputs.to(device), targets.to(device)\n",
        "      outputs= model(inputs, targets)\n",
        "      loss= loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
        "\n",
        "      nn.utils.clip_grad.clip_grad_norm_(model.parameters(), max_norm= clip)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss_train.update(loss.item(), n= len(targets))\n",
        "      if metric:\n",
        "        metric.update(outputs, targets)\n",
        "        metric_train_val= metric.compute().item()\n",
        "      else:\n",
        "        metric_train_val= None\n",
        "\n",
        "      tepoch.set_postfix(loss= loss_train.avg, metric= metric_train_val)\n",
        "\n",
        "    return model, loss_train.avg, metric_train_val"
      ],
      "metadata": {
        "id": "1BKvHjXKD2ui"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, loss_fn, metric= None):\n",
        "  model.eval()\n",
        "  loss_eval= AverageMeter()\n",
        "  if metric: metric.reset()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for inputs, targets in test_loader:\n",
        "      inputs, targets= inputs.to(device), targets.to(device)\n",
        "      outputs= model(inputs, targets)\n",
        "      loss= loss_fn(outputs.reshape(-1, outputs.shape[-1]), targets.flatten())\n",
        "      loss_eval.update(loss.item(), n= len(targets))\n",
        "      if metric: metric(outputs, targets)\n",
        "\n",
        "  return loss_eval.avg, metric.compute().item() if metric else None"
      ],
      "metadata": {
        "id": "12LWfUaltKm2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------"
      ],
      "metadata": {
        "id": "tH1_8JL-tf9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05_Experiments before the main training"
      ],
      "metadata": {
        "id": "nVCk0s85th9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## base loss"
      ],
      "metadata": {
        "id": "xk-OgRNgtoTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= ImageCaptioning(embed_size, hidden_size, len(caption_transform.vocab),\n",
        "                       num_layers, dropout_embd, dropout_rnn, max_seq_length).to(device)\n",
        "\n",
        "loss_base, _= evaluate(model, valid_loader, loss_fn)\n",
        "print(f'{loss_base:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQaGiJ-QthQu",
        "outputId": "df06afb0-d71f-43ff-d3df-98fe2fa43ac2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.097713442993165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## overfit on subset of data"
      ],
      "metadata": {
        "id": "LPMGxCTZtsbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs= 20\n",
        "mini_train_size= 1000\n",
        "\n",
        "optimizer= torch.optim.SGD(model.parameters(), lr= lr, momentum= momentum)\n",
        "_, mini_train_dataset= random_split(train_set, (len(train_set)- mini_train_size,\n",
        "                                                mini_train_size))\n",
        "mini_train_loader= DataLoader(mini_train_dataset, 20, collate_fn= collate_fn)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model, _, _= train_one_epoch(model, mini_train_loader, loss_fn, optimizer, None, epoch)\n",
        "\n",
        "del mini_train_dataset, mini_train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "aSrgwYZ0tggc",
        "outputId": "2a61dda2-e51d-4bc1-cc42-f0691d2d6250"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:12<00:00,  4.05batch/s, loss=7.43, metric=None]\n",
            "Epoch 1: 100%|██████████| 50/50 [00:13<00:00,  3.83batch/s, loss=5.27, metric=None]\n",
            "Epoch 2: 100%|██████████| 50/50 [00:09<00:00,  5.10batch/s, loss=4.81, metric=None]\n",
            "Epoch 3: 100%|██████████| 50/50 [00:09<00:00,  5.05batch/s, loss=4.64, metric=None]\n",
            "Epoch 4: 100%|██████████| 50/50 [00:10<00:00,  4.88batch/s, loss=4.55, metric=None]\n",
            "Epoch 5: 100%|██████████| 50/50 [00:10<00:00,  4.94batch/s, loss=4.47, metric=None]\n",
            "Epoch 6: 100%|██████████| 50/50 [00:09<00:00,  5.08batch/s, loss=4.39, metric=None]\n",
            "Epoch 7: 100%|██████████| 50/50 [00:09<00:00,  5.19batch/s, loss=4.24, metric=None]\n",
            "Epoch 8: 100%|██████████| 50/50 [00:10<00:00,  4.86batch/s, loss=4.11, metric=None]\n",
            "Epoch 9: 100%|██████████| 50/50 [00:10<00:00,  4.95batch/s, loss=4.02, metric=None]\n",
            "Epoch 10: 100%|██████████| 50/50 [00:09<00:00,  5.10batch/s, loss=3.91, metric=None]\n",
            "Epoch 11: 100%|██████████| 50/50 [00:10<00:00,  4.73batch/s, loss=3.79, metric=None]\n",
            "Epoch 12: 100%|██████████| 50/50 [00:10<00:00,  4.94batch/s, loss=3.67, metric=None]\n",
            "Epoch 13: 100%|██████████| 50/50 [00:10<00:00,  4.95batch/s, loss=3.57, metric=None]\n",
            "Epoch 14: 100%|██████████| 50/50 [00:10<00:00,  4.82batch/s, loss=3.47, metric=None]\n",
            "Epoch 15: 100%|██████████| 50/50 [00:09<00:00,  5.22batch/s, loss=3.38, metric=None]\n",
            "Epoch 16: 100%|██████████| 50/50 [00:10<00:00,  4.98batch/s, loss=3.29, metric=None]\n",
            "Epoch 17: 100%|██████████| 50/50 [00:10<00:00,  4.96batch/s, loss=3.21, metric=None]\n",
            "Epoch 18: 100%|██████████| 50/50 [00:09<00:00,  5.23batch/s, loss=3.13, metric=None]\n",
            "Epoch 19: 100%|██████████| 50/50 [00:09<00:00,  5.15batch/s, loss=3.06, metric=None]\n",
            "Epoch 20: 100%|██████████| 50/50 [00:09<00:00,  5.01batch/s, loss=2.99, metric=None]\n",
            "Epoch 21: 100%|██████████| 50/50 [00:10<00:00,  4.99batch/s, loss=2.94, metric=None]\n",
            "Epoch 22:  24%|██▍       | 12/50 [00:02<00:07,  4.96batch/s, loss=2.85, metric=None]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-5fb51418e42e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmini_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-ad8331d2a290>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, loss_fn, optimizer, metric, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-e58a46a5fdc3>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mimg_raw\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rough grid"
      ],
      "metadata": {
        "id": "Iod-YiNf-2HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs= 1\n",
        "loss_grid= loss_base\n",
        "\n",
        "for lr in [0.9, 0.5, 0.125, 0.005]:\n",
        "  print(f'LR={lr}')\n",
        "\n",
        "  model= ImageCaptioning(embed_size, hidden_size, len(caption_transform.vocab),\n",
        "                         num_layers, dropout_embd, dropout_rnn, max_seq_length).to(device)\n",
        "  # model= torch.load('model.pt')\n",
        "  optimizer = optim.SGD(model.parameters(), lr= lr,\n",
        "                        weight_decay= wd, momentum= momentum)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model, loss, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, None, epoch+1)\n",
        "  if loss< loss_grid:\n",
        "    best_lr= lr\n",
        "    loss_grid= loss\n",
        "    print(f'best loss is: {loss_grid} with lr: {best_lr}')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6TFIoHdD-mb",
        "outputId": "f67ef99b-372a-4240-b6ee-3ce44632ba7b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR=0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 235/235 [04:43<00:00,  1.21s/batch, loss=4.43, metric=None]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best loss is: 4.433724770100912 with lr: 0.9\n",
            "\n",
            "LR=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 235/235 [04:43<00:00,  1.21s/batch, loss=4.63, metric=None]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LR=0.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 235/235 [04:44<00:00,  1.21s/batch, loss=5.31, metric=None]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LR=0.005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 235/235 [04:43<00:00,  1.21s/batch, loss=8.36, metric=None]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## grid search"
      ],
      "metadata": {
        "id": "5Rg77aLzCF-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs= 2\n",
        "lr= best_lr\n",
        "\n",
        "for wd in [1e-4, 0]:\n",
        "  print(f'LR={lr}, WD={wd}')\n",
        "\n",
        "  model= ImageCaptioning(embed_size, hidden_size, len(caption_transform.vocab),\n",
        "                         num_layers, dropout_embd, dropout_rnn, max_seq_length).to(device)\n",
        "  # model= torch.load('model.pt')\n",
        "  optimizer = optim.SGD(model.parameters(), lr= lr,\n",
        "                        weight_decay= wd, momentum= momentum)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model, _, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, None, epoch+1)\n",
        "  loss_valid, _= evaluate(model, valid_loader, loss_fn, None)\n",
        "  print(f'Valid: Loss= {loss_valid:.4}')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "yzsOkn-NCHMT",
        "outputId": "0b072990-5002-4fd1-b63e-e34b4e35a0fe"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR=0.9, WD=0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  67%|██████▋   | 158/235 [03:12<01:33,  1.22s/batch, loss=4.79, metric=None]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a6fdad282454>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Valid: Loss= {loss_valid:.4}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-ad8331d2a290>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, loss_fn, optimizer, metric, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-e58a46a5fdc3>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mimg_raw\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------"
      ],
      "metadata": {
        "id": "haUfeALSFlzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06_Training"
      ],
      "metadata": {
        "id": "KXbkDkvmFm2j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6NzODI0FmaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}